# Incident: 2025-12-06 19-45-20 UTC

## Summary

Between 19:45 and 20:00 UTC on 2025-12-06, the JWT Pizza service experienced a critical failure in order fulfillment. The incident was triggered by the Pizza Factory entering a chaotic state, rejecting all requests with a "chaos monkey" error message. This resulted in 739 failed pizzas and affected 200 separate customer orders.

The event was detected immediately by the `failedPizzas_total` metric alert and the `chaosUrl_total` metric alert. The on-call engineer (Spencer Clingo) diagnosed the issue by inspecting the Chaos Urls metric table and resolved it by following the URL provided in the Pizza Factory response. The impact lasted approximately 15 minutes.

## Detection

The incident was detected by an alert at **19:46:00 UTC**, exactly 40 seconds after the first failure was reported. The detection occurred via a Grafana alert on the `chaosUrl_total` metric, which paged the on-call engineer of a new entry in the data.

Time-to-detection was highly effective (under 1 minute). To improve this further, we could tighten the alert evaluation window, though this risks increasing false positives. Current detection speed is considered optimal.

## Impact

For 15 minutes between 19:45:20 and 20:00:20 UTC on 2025-12-06, the JWT Pizza creation service was unable to fulfill orders.

* **External Impact:** 200 customer orders were processed but failed to generate pizzas.
* **Internal Impact:** 739 pizzas failed creation, logging errors across the backend system.
* **Support:** Hypothetically, this would have generated 200 immediate support tickets regarding unfulfilled orders. If we had a system for it, many of the users hopefully would have created support tickets manually as well.

## Timeline

> [!Note] All times are reported in UTC.

* _19:45:20_ - **Incident Start:** The JWT Pizza Factory service began rejecting requests with the error message "chaos monkey", a status code of 500, and a reportUrl.
* _19:46:00_ - **Detection:** Grafana alert `New Chaos Url` fired; on-call engineer notified.
* _19:57:30_ - **Investigation:** Engineer accessed Grafana logs and identified the specific "chaos monkey" error message in the response body.
* _19:58:00_ - **Diagnosis:** Engineer navigated to the Chaos Url table, copying the reported URL coming from the urlToEndChaos parameter.
* _19:58:30_ - **Mitigation:** Engineer manually triggered the `urlToEndChaos` endpoint provided in the table.
* _20:00:20_ - **Recovery:** Metrics for pizza creation returned to nominal levels; 200 OK responses observed.
* _20:03:00_ - **Alert Resolution:** The Grafana alert automatically resolved as error rates dropped to zero.

## Response

After receiving the page at 19:46:00 UTC, the on-call DevOps engineer Spencer Clingo came online. He was about to leave for a haircut when he noticed the alerts, but dropped everything and inspected the Grafana dashboard to confirm the spike in failures.

There were no significant delays. The response was straightforward once the specific URL was located in the Pizza Factory responses and in the Chaos URL table.

Due to the Chaos URL table, once the errors were noticed, the issue was resolved immediately. The only way the errors could have been resolved faster would have been an automatic curl request to the provided URL.

## Root cause

The root cause was an intentional chaos event triggered by the upstream Pizza Factory provider. The provider entered a failure mode where it rejected valid pizza creation requests to test the observability of the system.

## Resolution

The service was restored by following the URL that the vendor provided in the rejection responses. After following the URL, metrics continued to be tracked to verify that the error responses dropped to zero and that there were no more pizza creation failures.

There are two ways the issue could have been resolved faster:
1. The on-call engineer was watching his notifications more closely, reacting in just a few minutes instead of nearly 15.
2. When the Pizza Factory started sending the URL, the service could have automatically made a curl request to the URL, ending the Chaos as fast as it started.

## Prevention

This specific failure mode is part of a planned chaos testing suite. To prevent user impact in the future, the application could implement a "Circuit Breaker" pattern. If the Factory returns a "chaos monkey" error, the system could automatically pause orders or switch to a backup provider (if available) rather than attempting 200 failing requests.

## Action items

1.  **Update Runbooks:** Document the "chaos monkey" error specifically in the on-call runbook, noting that the resolution link is found inside the error payload.
2.  **Pause Orders:** Instead of continuing to allow users to make orders while the system is down, we could pause order creations and notify users that orders will be able to be made momentarily.